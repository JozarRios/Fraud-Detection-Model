# -*- coding: utf-8 -*-
"""Fraud Detection by Jozar Rios

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GhR3FbJ-YYuhuanuEb15afBxaWHH4ZhK

### Machine Learning Model:
#**Project: Fraud Detection**

**Fraud detection** is a critical application of machine learning and data analytics, particularly in domains where financial transactions or sensitive data are involved.

One of the ***common challenges*** faced by the insurance industry is ***detecting fraudulent claims***, which can lead to significant financial losses. Machine learning algorithms can be used to identify patterns and anomalies in data that may indicate fraudulent behavior.

In this project, we will focus on fraud detection for vehicle insurance claims using a dataset from Kaggle's Vehicle Claim Fraud Detection. Our objective is to build a predictive model that can help distinguish between genuine and fraudulent insurance claims.

###**Objectives**
- **Data Exploration:** <br>
Understand the dataset by performing initial exploration and visualization.
- **Data Preprocessing:** <br>Clean and prepare the dataset for modeling by handling missing values, encoding categorical variables, and normalizing features.
- **Feature Engineering:** <br>Identify and create features that will improve the model's predictive power.
- **Model Training:** <br>Develop a machine learning model using algorithms like Logistic Regression, Decision Trees, or Random Forest.
- **Evaluation:** <br>Evaluate the model's performance using accuracy, precision, recall, and the F1-score to understand its effectiveness in detecting fraud.
- **Deployment:** <br>Prepare the notebook to be a reference for further improvements, such as testing more advanced algorithms or hyperparameter tuning.

### Dataset Description

The Kaggle dataset contains information about <a href='https://www.kaggle.com/datasets/shivamb/vehicle-claim-fraud-detection'> vehicle insurance claims </a> , with features such as:

1. **Policy Information:**

- `PolicyType`, `VehicleCategory`, `VehiclePrice`, `BasePolicy`, and `Deductible`: These columns can provide insights into the types of policies and the insured vehicle's value, which may correlate with fraudulent claims.

2. **Demographic Information:**

- `Sex`, `MaritalStatus`, `Age`, `AgeOfPolicyHolder`, and `AddressChange_Claim`: These demographic features might help identify patterns related to fraud.

3. **Claim Information:**

- `Month`, `WeekOfMonth`, `DayOfWeek`, `MonthClaimed`, `WeekOfMonthClaimed`, `DayOfWeekClaimed`: These time-based columns can be useful for detecting suspicious patterns in claim timing.
AccidentArea, Fault, and PoliceReportFiled: Details about the accident can be indicative of legitimate or fraudulent claims.
WitnessPresent, AgentType, NumberOfSuppliments: The presence of witnesses, the type of agent handling the case, and the number of additional documents might provide clues.

4. **Vehicle and Driver Details:**

- `Make`, `AgeOfVehicle`, and `DriverRating`: Information related to the vehicle's make and condition, as well as the driver's record, may influence the likelihood of fraud.
PastNumberOfClaims: A history of multiple claims could be a potential indicator of fraud.

5. **Fraud Label**:

- `FraudFound_P`: The target column indicating whether the claim was fraudulent (1) or not (0).

###Tools & Technologies

We will be using Python in a Jupyter Notebook environment for this project, utilizing libraries such as:

- Pandas for data manipulation.
- Matplotlib and Seaborn for visualization.
- Scikit-learn for machine learning algorithms.
- NumPy for numerical computations.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns


from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

df = pd.read_csv('/content/fraud_oracle.csv')
display(df)

"""#Information of the dataset"""

print(df.head())

df.nunique()

df.info()

# prompt: display the unique values name in every columns of df

for col in df.columns:
    print(f"Unique values in column '{col}':")
    print(df[col].unique())
    print("-" * 20)

"""#Handling Missing Values base on the mean of the dataset"""

# Handling missing values (Example: replacing with mode/median)
#df['AgeOfVehicle'].fillna(df['AgeOfVehicle'].mode()[0], inplace=True)
#df['AgeOfPolicyHolder'].fillna(df['AgeOfPolicyHolder'].median(), inplace=True)

# Encode categorical variables using Label Encoding
label_enc = LabelEncoder()
for col in df.select_dtypes(include=['object']).columns:
    df[col] = label_enc.fit_transform(df[col])

# Scaling numeric features
scaler = StandardScaler()
num_cols = ['Age', 'Deductible', 'DriverRating', 'RepNumber', 'PastNumberOfClaims']
df[num_cols] = scaler.fit_transform(df[num_cols])

"""#Feature Correlation

"""

# Define features (X) and target (y)
X = df.drop(columns=['FraudFound_P', 'PolicyNumber'])
y = df['FraudFound_P']

# Optional: Check feature correlation
plt.figure(figsize=(24, 16))
sns.heatmap(df.corr(), annot=True, fmt=".2f")
plt.title('Feature Correlation Matrix')
plt.show()

#Splitting the df into training set and testing sets to evaluates model performance
#(80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#use classification algorithm (random forest) to train the model
#start the RandomForestClassifier
model = RandomForestClassifier(n_estimators=100, random_state=42)

#train the model
model.fit(X_train, y_train)

#predict on the test set
y_pred = model.predict(X_test)

#we will use metrics like accuracy, precision, recall, and F1-score, focusing on precision and recall
#they provide a wider picture of fraud detection capabilities.

#metrics
print("Accuracy Score:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

#ploting the confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Fraud', 'Fraud'], yticklabels=['Not Fraud', 'Fraud'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

from sklearn.metrics import precision_recall_curve, auc

# Get predicted probabilities
y_prob = model.predict_proba(X_test)[:, 1]  # Probability of fraud class

precision, recall, thresholds = precision_recall_curve(y_test, y_prob)
auc_score = auc(recall, precision)

# Plot the Precision-Recall curve
plt.plot(recall, precision, label=f'Precision-Recall AUC: {auc_score:.2f}')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.legend()
plt.title('Precision-Recall Curve for Fraud Detection')
plt.show()

#shows feature importance
feature_importance = pd.Series(model.feature_importances_, index=X.columns)
feature_importance.sort_values(ascending=False).plot(kind='bar', figsize=(12, 8))
plt.title('Feature Importance')
plt.show()

from sklearn.model_selection import GridSearchCV

# Parameter grid for RandomForest
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10]
}

# Grid search with cross-validation
grid_search = GridSearchCV(estimator=RandomForestClassifier(random_state=42), param_grid=param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)

# Best parameters found
print("Best Parameters:", grid_search.best_params_)

"""#We will export the model, to make the model accessible outside of Python using Pickle"""

import pickle

# Save the trained model
with open('fraud_detection_model.pkl', 'wb') as f:
    pickle.dump(model, f)

#download the file

from google.colab import files

files.download('fraud_detection_model.pkl')

"""## Let's put it in the Gradio"""

#we will install gradio
!pip install gradio

# let's save the trained Random Forest model
#using joblib or pickle. This way, you don't need
#to train the model every time you run the Gradio interface.

import joblib

# Save the trained model
joblib.dump(model, 'fraud_detection_model.pkl')

# Load the saved model (for demonstration)
model = joblib.load('fraud_detection_model.pkl')

# Function to make predictions
def predict_fraud(
    Month, WeekOfMonth, DayOfWeek, Make, AccidentArea, DayOfWeekClaimed, MonthClaimed,
    WeekOfMonthClaimed, Sex, MaritalStatus, Age, Fault, PolicyType, VehicleCategory,
    VehiclePrice, Deductible, DriverRating, Days_Policy_Accident, Days_Policy_Claim,
    PastNumberOfClaims, AgeOfVehicle, AgeOfPolicyHolder, PoliceReportFiled, WitnessPresent,
    AgentType, NumberOfSuppliments, AddressChange_Claim, NumberOfCars, Year, BasePolicy):

    # Load the pre-trained model
    model = joblib.load('fraud_detection_model.pkl')

    # Create a DataFrame with the input data
    input_data = pd.DataFrame({
        'Month': [Month], 'WeekOfMonth': [WeekOfMonth], 'DayOfWeek': [DayOfWeek],
        'Make': [Make], 'AccidentArea': [AccidentArea], 'DayOfWeekClaimed': [DayOfWeekClaimed],
        'MonthClaimed': [MonthClaimed], 'WeekOfMonthClaimed': [WeekOfMonthClaimed],
        'Sex': [Sex], 'MaritalStatus': [MaritalStatus], 'Age': [Age], 'Fault': [Fault],
        'PolicyType': [PolicyType], 'VehicleCategory': [VehicleCategory], 'VehiclePrice': [VehiclePrice],
        'Deductible': [Deductible], 'DriverRating': [DriverRating], 'Days_Policy_Accident': [Days_Policy_Accident],
        'Days_Policy_Claim': [Days_Policy_Claim], 'PastNumberOfClaims': [PastNumberOfClaims],
        'AgeOfVehicle': [AgeOfVehicle], 'AgeOfPolicyHolder': [AgeOfPolicyHolder],
        'PoliceReportFiled': [PoliceReportFiled], 'WitnessPresent': [WitnessPresent],
        'AgentType': [AgentType], 'NumberOfSuppliments': [NumberOfSuppliments],
        'AddressChange_Claim': [AddressChange_Claim], 'NumberOfCars': [NumberOfCars],
        'Year': [Year], 'BasePolicy': [BasePolicy]
    })

    # Encode and scale input_data as you did during training
    # (Assuming you have LabelEncoder and StandardScaler used during training)

    # Encode categorical columns
    #for col in input_data.select_dtypes(include=['object']).columns:
    #   input_data[col] = label_enc.fit_transform(input_data[col])

    # Scale numerical columns
    #input_data[num_cols] = scaler.transform(input_data[num_cols])

    # Predict using the model
    prediction = model.predict(input_data)

    # Return result
    return "Fraud Detected" if prediction[0] == 1 else "No Fraud Detected"

"""Gradio Interface:"""

import gradio as gr

# Define the Gradio interface
interface = gr.Interface(
    fn=predict_fraud,
    inputs=[
        gr.Dropdown(choices=['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'], label='Month'),
        gr.Dropdown(choices=[1, 2, 3, 4, 5], label='Week of Month'),
        gr.Dropdown(choices=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'], label='Day of Week'),
        gr.Textbox(label='Make'),
        gr.Dropdown(choices=['Urban', 'Rural'], label='Accident Area'),
        gr.Dropdown(choices=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'], label='Day of Week Claimed'),
        gr.Dropdown(choices=['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'], label='Month Claimed'),
        gr.Slider(minimum=1, maximum=5, value=1, label='Week of Month Claimed'),
        gr.Dropdown(choices=['Male', 'Female'], label='Sex'),
        gr.Dropdown(choices=['Single', 'Married', 'Widow', 'Divorced'], label='Marital Status'),
        gr.Slider(minimum=18, maximum=100, value=30, label='Age'),
        gr.Dropdown(choices=['Policy Holder', 'Third Party'], label='Fault'),
        gr.Textbox(label='Policy Type'),
        gr.Textbox(label='Vehicle Category'),
        gr.Dropdown(choices=['<20000', '20000-29000', '30000-39000', '40000-59000', '>60000'], label='Vehicle Price'),
        gr.Slider(minimum=0, maximum=1000, value=200, label='Deductible'),
        gr.Slider(minimum=1, maximum=5, value=3, label='Driver Rating'),
        gr.Textbox(label='Days Policy Accident'),
        gr.Textbox(label='Days Policy Claim'),
        gr.Textbox(label='Past Number of Claims'),
        gr.Textbox(label='Age of Vehicle'),
        gr.Slider(minimum=18, maximum=100, value=40, label='Age of Policy Holder'),
        gr.Dropdown(choices=['Yes', 'No'], label='Police Report Filed'),
        gr.Dropdown(choices=['Yes', 'No'], label='Witness Present'),
        gr.Dropdown(choices=['External', 'Internal'], label='Agent Type'),
        gr.Textbox(label='Number of Supplements'),
        gr.Textbox(label='Address Change Claim'),
        gr.Textbox(label='Number of Cars'),
        gr.Slider(minimum=1990, maximum=2024, value=2020, label='Year'),
        gr.Dropdown(choices=['All Perils', 'Collision', 'Liability'], label='Base Policy')
    ],
    outputs=gr.Textbox(label="Fraud Prediction")
)

# Launch the Gradio interface
interface.launch()

"""#Exposing the model to Flask API

"""

!pip install flask joblib pandas scikit-learn
from flask import Flask, request, jsonify
import joblib

app = Flask(__name__)

#load the model
model = joblib.load('fraud_detection_model.pkl')

#define a route for prediction
@app.route('/predict', methods=['POST'])
def predict():
    try:
        #get data from the request
        data = request.get_json()

        #create a DataFrame from the input data

        input_data = pd.DataFrame([data])

        #make a prediction
        prediction = model.predict(input_data)[0]

        #return the prediction as a JSON response
        return jsonify({'prediction': int(prediction)})  #will rreturn as integer

    except Exception as e:
        return jsonify({'error': str(e)})

if __name__ == '__main__':
    app.run(debug=True, host="0.0.0.0") #allow external

